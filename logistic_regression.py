#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Mar 15 19:20:48 2022@author: shivanshsuhane"""import pandas as pdimport numpy as npimport randomfrom sklearn.preprocessing import StandardScalerdef printRunStatistics(preds, labels):    print("Run Statistics: ")        TP=0    FP=0    FN=0    TN=0    accu=0            for i in range (len(labels)):        label = labels[i]        pred = preds[i]        if label==1 and label==pred:            TP+=1            accu+=1        elif label==0 and label==pred:            TN+=1            accu+=1        elif label==1 and label!=pred:            FN+=1        elif label==0 and label!=pred:            FP+=1        precision=TP/(TP+FP)    recall=TP/(TP+FN)    FVal = 2*precision*recall/(precision+recall)    accu=accu/np.size(preds,0)    print("Accuracy :"+ str(accu))    print("Recall: "+ str(recall))    print("Precision: "+str(precision))    print("f-measure: "+str(FVal))def log_likelihood(features, target, weights):    features = np.append(arr = np.ones([features.shape[0],1]).astype(int), values = features,axis=1)  #concatenate the ones to train matrix    scores = np.dot(features, weights)    var1 = target*scores - np.log(1 + np.exp(scores))    ll = np.sum( var1 )    return lldef getPredictedValues(weights, X):    X = np.append(arr = np.ones([X.shape[0],1]).astype(int), values = X,axis=1)  #concatenate the ones to train matrix    pred_outputs = np.matmul(X, weights)    return pred_outputsdef sigmoid(x):    # Activation function used to map any real value between 0 and 1    return 1 / (1 + np.exp(-x))def logReg(trainMat, yTrain, stepLimit, learning_rate):    trainMat = np.append(arr = np.ones([trainMat.shape[0],1]).astype(int), values = trainMat,axis=1)  #concatenate the ones to train matrix    dim = trainMat.shape[1]    theta = np.zeros((dim,1))    size = np.size(trainMat,0)            i=0    while i< stepLimit:        scores = np.dot(trainMat,theta)        predictions = sigmoid(scores)                #gradient ascent        output = (np.asarray(yTrain).T-np.asarray(predictions).T).T                gradient = (1/size)*np.dot(trainMat.T,output)        theta+= learning_rate*gradient        i+=1            print('model ran successfully')    return thetadef one_hot_encode(column_name,df):    dummy_col = pd.get_dummies(df[column_name], prefix=column_name+'_')        df = pd.merge(        left=df,        right=dummy_col,        left_index=True,        right_index=True,    )    df = df.drop(column_name, 1)        return dfif __name__ == "__main__":        #1) read in data    df = pd.read_csv ('nan_removed.csv')    df = df.drop('JOB', 1)            l = ['HomeImp','DebtCon']    df['REASON'].fillna(random.choice(l), inplace=True)        ##randomize_one_hot(df)    df=one_hot_encode('REASON',df)    print(df.head())        #standardizing the column values for selective columns    standardize_cols=["LOAN", "MORTDUE","VALUE","YOJ","DEROG", "DELINQ", "CLAGE", "NINQ","CLNO", "DEBTINC"]    scaler = StandardScaler()    scaler.fit(df[standardize_cols])    df[standardize_cols] = scaler.transform(df[standardize_cols])        #2) randomize the data    data = df.to_numpy()    np.random.shuffle(data)    print(df[:5])        #3) split to test and train datasets    trainCount = int(len(data)*3/4)    trainMat, testMat = data[:trainCount, :],data[trainCount:,:]    #https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros        yTrain = np.asarray(trainMat[:,0])    #get values for train y    yTest = testMat[:,0]     #get values for test y        #delete y values from original matrices    testMat = np.delete(testMat,0,1)       trainMat = np.delete(trainMat,0,1)         #add back the y values for sample split    trainMat = np.column_stack((trainMat, yTrain))        #5) Divides the training data into two groups: badSamples, notBadSamples    trainbadSamples = trainMat[trainMat[:,-1]==1]    ybadSamples = trainbadSamples[:,-1]    trainbadSamples=np.delete(trainbadSamples,-1,1)          trainNotBadSamples = trainMat[trainMat[:,-1]==0]    yNotBadSamples= trainNotBadSamples[:,-1]    trainNonSpamSamples=np.delete(trainNotBadSamples,-1,1)        trainMat = np.delete(trainMat,-1,1)     #Weights = linReg(trainMat, yTrain)            stepLimit = 30000    learning_rate = 0.5    #10*math.exp(-7)            Weights = logReg(trainMat, yTrain, stepLimit, learning_rate)        yPredictions = getPredictedValues(Weights,testMat)    yPredictionsFinal = log_likelihood(testMat,yTest,Weights)    yPredictions = yPredictions > 0.5            #8) now we have the predictions array, we compare it with original to get our model statistics        printRunStatistics(yPredictions, yTest)                        