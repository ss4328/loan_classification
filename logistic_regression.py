#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Mar 15 19:20:48 2022@author: shivanshsuhane"""import pandas as pdimport numpy as npimport randomfrom sklearn.preprocessing import MinMaxScaler, RobustScalerfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import cross_validatefrom sklearn import metricsfrom sklearn.pipeline import Pipelinedef one_hot_encode(column_name,df):    dummy_col = pd.get_dummies(df[column_name], prefix=column_name+'_')        df = pd.merge(        left=df,        right=dummy_col,        left_index=True,        right_index=True,    )    df = df.drop(column_name, 1)        return dfif __name__ == "__main__":        #1) read in data    df = pd.read_csv ('nan_removed.csv')    df = df.drop('JOB', 1)            l = ['HomeImp','DebtCon']    df['REASON'].fillna(random.choice(l), inplace=True)        ##randomize_one_hot(df)    df=one_hot_encode('REASON',df)    df.info()    df.describe()    df.corr()        #standardizing the column values for selective columns    # standardize_cols=["LOAN", "MORTDUE","VALUE","YOJ","DEROG", "DELINQ", "CLAGE", "NINQ","CLNO", "DEBTINC"]    # scaler = StandardScaler()    # scaler.fit(df[standardize_cols])    # df[standardize_cols] = scaler.transform(df[standardize_cols])    trans = RobustScaler(with_centering=False, with_scaling=True)        #2) randomize the data    data = df.to_numpy()    np.random.shuffle(data)    print(df[:5])        #3) split to test and train datasets    trainCount = int(len(data)*3/4)    trainMat, testMat = data[:trainCount, :],data[trainCount:,:]    #https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros        yTrain = np.asarray(trainMat[:,0])    #get values for train y    yTest = testMat[:,0]     #get values for test y        #delete y values from original matrices    testMat = np.delete(testMat,0,1)       trainMat = np.delete(trainMat,0,1)         #add back the y values for sample split    trainMat = np.column_stack((trainMat, yTrain))        #5) Divides the training data into two groups: badSamples, notBadSamples    trainbadSamples = trainMat[trainMat[:,-1]==1]    ybadSamples = trainbadSamples[:,-1]    trainbadSamples=np.delete(trainbadSamples,-1,1)          trainNotBadSamples = trainMat[trainMat[:,-1]==0]    yNotBadSamples= trainNotBadSamples[:,-1]    trainNonSpamSamples=np.delete(trainNotBadSamples,-1,1)        trainMat = np.delete(trainMat,-1,1)     #Weights = linReg(trainMat, yTrain)                logistic = LogisticRegression(max_iter=10000, tol=0.1)        pipe = Pipeline(steps=[("scaler", trans), ("logistic", logistic)])    pipe.fit(trainMat, yTrain)        print("{} Test Accuracy: {}".format('logistic reg',logistic.score(testMat,yTest)))        scores = cross_validate(pipe, trainMat, yTrain)    print(scores)        logistic.fit(trainMat, yTrain)    y_pred=pipe.predict(testMat)    cnf_matrix = metrics.confusion_matrix(yTest, y_pred)    cnf_matrix        #printing scores    for i in range(5):        print()    print("Accuracy:",metrics.accuracy_score(yTest, y_pred))    print("Precision:",metrics.precision_score(yTest, y_pred))    print("Recall:",metrics.recall_score(yTest, y_pred))    for i in range(5):        print()                